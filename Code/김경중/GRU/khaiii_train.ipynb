{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "khaiii_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0FSslKGan64EvkCj2EjU4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_1NfW9281jt",
        "outputId": "ab56707d-edd1-4832-98e3-8df103794b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4AFNN_gF-ep",
        "outputId": "109d8aba-7b5d-45c1-a908-38d8a57b590a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/kakao/khaiii.git\n",
        "\n",
        "!pip install cmake\n",
        "\n",
        "!mkdir build\n",
        "\n",
        "!cd build && cmake /content/khaiii\n",
        "\n",
        "!cd /content/build/ && make all\n",
        "\n",
        "!cd /content/build/ && make resource\n",
        "\n",
        "!cd /content/build && make install\n",
        "\n",
        "!cd /content/build && make package_python\n",
        "\n",
        "!pip install /content/build/package_python\n",
        "!pip install koco"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'khaiii' already exists and is not an empty directory.\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "mkdir: cannot create directory ‘build’: File exists\n",
            "-- [khaiii] fused multiply add option enabled\n",
            "-- [hunter] Calculating Toolchain-SHA1\n",
            "-- [hunter] Calculating Config-SHA1\n",
            "-- [hunter] HUNTER_ROOT: /root/.hunter\n",
            "-- [hunter] [ Hunter-ID: 70287b1 | Toolchain-ID: 02ccb06 | Config-ID: dffbc08 ]\n",
            "-- [hunter] BOOST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.68.0-p1)\n",
            "-- Boost version: 1.68.0\n",
            "-- [hunter] CXXOPTS_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 2.1.1-pre)\n",
            "-- [hunter] EIGEN_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.5)\n",
            "-- [hunter] FMT_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 4.1.0)\n",
            "-- [hunter] GTEST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.8.0-hunter-p11)\n",
            "-- [hunter] NLOHMANN_JSON_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.0)\n",
            "-- [hunter] SPDLOG_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 0.16.3-p1)\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/build\n",
            "[ 65%] Built target obj_khaiii\n",
            "[ 69%] Built target khaiii\n",
            "[ 76%] Built target bin_khaiii\n",
            "[100%] Built target test_khaiii\n",
            "Built target resource\n",
            "[ 65%] Built target obj_khaiii\n",
            "[ 69%] Built target khaiii\n",
            "[ 76%] Built target bin_khaiii\n",
            "[100%] Built target test_khaiii\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"\"\n",
            "-- Up-to-date: /usr/local/include/khaiii\n",
            "-- Up-to-date: /usr/local/include/khaiii/khaiii_dev.h\n",
            "-- Up-to-date: /usr/local/include/khaiii/KhaiiiApi.hpp\n",
            "-- Up-to-date: /usr/local/include/khaiii/khaiii_api.h\n",
            "-- Up-to-date: /usr/local/share/khaiii\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.3.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/preanal.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/cnv2hdn.lin\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.key\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.5.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.one\n",
            "-- Up-to-date: /usr/local/share/khaiii/preanal.tri\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.4.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/hdn2tag.lin\n",
            "-- Up-to-date: /usr/local/share/khaiii/restore.val\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.len\n",
            "-- Up-to-date: /usr/local/share/khaiii/conv.2.fil\n",
            "-- Up-to-date: /usr/local/share/khaiii/errpatch.tri\n",
            "-- Up-to-date: /usr/local/share/khaiii/embed.bin\n",
            "-- Up-to-date: /usr/local/share/khaiii/config.json\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so.0.4\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so.0\n",
            "-- Up-to-date: /usr/local/lib/libkhaiii.so\n",
            "-- Up-to-date: /usr/local/bin/khaiii\n",
            "\u001b[36mRun CPack packaging tool for source...\u001b[0m\n",
            "CPack: Create package using ZIP\n",
            "CPack: Install projects\n",
            "CPack: - Install directory: /content/khaiii\n",
            "CPack: Create package\n",
            "CPack: - package: /content/build/khaiii-0.4.zip generated.\n",
            "Built target package_python\n",
            "Processing ./build/package_python\n",
            "Building wheels for collected packages: khaiii\n",
            "  Building wheel for khaiii (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for khaiii: filename=khaiii-0.4-cp36-none-any.whl size=22882400 sha256=4231927d2eca1aa83319d863eb4a97f572eac565be96fd7d07f629e6d8c6951b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-of0l4k0i/wheels/6b/4d/23/84c4acc3ef5226ca75c4e3ad84c39a0654d11c3a9c1941193f\n",
            "Successfully built khaiii\n",
            "Installing collected packages: khaiii\n",
            "  Found existing installation: khaiii 0.4\n",
            "    Uninstalling khaiii-0.4:\n",
            "      Successfully uninstalled khaiii-0.4\n",
            "Successfully installed khaiii-0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "khaiii"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: koco in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.6/dist-packages (from koco) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from koco) (1.1.2)\n",
            "Requirement already satisfied: wget>=3.2 in /usr/local/lib/python3.6/dist-packages (from koco) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->koco) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->koco) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->koco) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.23.0->koco) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.3->koco) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.3->koco) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.3->koco) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.3->koco) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upyCypBc-i3e"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "#주요 참고 PyTorch로 시작하는 딥 러닝 입문, 유원준\n",
        "from torchtext import data  \n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import random\n",
        "import koco"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS89tSSfGDIe"
      },
      "source": [
        "from khaiii import KhaiiiApi\n",
        "api = KhaiiiApi()\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1P4WcqH4fwY"
      },
      "source": [
        "train_dev = koco.load_dataset('korean-hate-speech', mode='train_dev')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsgKUh2aRh_z"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('/content/gdrive/My Drive/datas/hate.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['document', 'lable']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for train in train_dev['train']:\n",
        "        lable = 1\n",
        "        if train['hate'] != 'none':\n",
        "            lable = 0\n",
        "        writer.writerow({'document':train['comments'],'lable':lable})\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7T1NhRtaV8J"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTcV2aB8URYE"
      },
      "source": [
        "#데이터 전처리 후에 남은 문자들이 전부 빈칸일 때 제거하기 위함\n",
        "def isAll0(x):\n",
        "    if type(x) == float:\n",
        "        return x\n",
        "    elif len(x) == x.count(' '):\n",
        "        return ''\n",
        "    else:\n",
        "        return x\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C4BIvfK-e8P",
        "outputId": "2432c43a-5979-4493-a087-0350f5b69621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "'''\n",
        "#데이터 가져와서\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "'''\n",
        "#읽고\n",
        "all_df = pd.read_csv('/content/gdrive/My Drive/datas/hate.csv')\n",
        "train_df = all_df[:int(len(all_df)*0.9)]\n",
        "test_df = all_df[int(len(all_df)*0.9):]\n",
        "\n",
        "#학습 데이터 전처리, 영어 특수문자 제거, 공백 제거\n",
        "train_df['document'] = train_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_df['document'] = train_df['document'].apply(isAll0)\n",
        "train_df['document'].replace('', np.nan, inplace=True)\n",
        "\n",
        "#테스트 데이터 전처리\n",
        "test_df['document'] = test_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "test_df['document'] = test_df['document'].apply(isAll0)\n",
        "test_df['document'].replace('', np.nan, inplace=True)\n",
        "\n",
        "#중복 데이터 제거\n",
        "train_df.drop_duplicates(subset=['document'], inplace=True)\n",
        "test_df.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "#Null 제거\n",
        "train_df = train_df.dropna(how = 'any')\n",
        "test_df = test_df.dropna(how = 'any')\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:4569: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU4xqF6e-uid",
        "outputId": "18227a83-f75e-4869-8f3b-0e015bbc3747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(train_df))\n",
        "print(len(test_df))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7104\n",
            "790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhuXSXriVmi"
      },
      "source": [
        "#불용어\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EaPAX08WscC"
      },
      "source": [
        "train_x = []"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulbculq9CpTm"
      },
      "source": [
        "#학습 데이터 형태소로 분리, 문장 시작, 끝 추가\n",
        "j = 0\n",
        "for i, row in train_df.iterrows():\n",
        "    val = row['document']\n",
        "    j += 1\n",
        "    if j % 10000 == 0:\n",
        "        print(i,\"/\",\"143660\")\n",
        "\n",
        "    temp = ['CLS']\n",
        "    for word in api.analyze(val):\n",
        "        for morph in word.morphs:\n",
        "            if morph.lex not in stopwords:\n",
        "                temp.append(morph.lex)\n",
        "    temp.append('SEP')\n",
        "    train_x.append(temp)\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx5Dv-cTXq4C"
      },
      "source": [
        "test_x = []"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFvyyGGIX9JY"
      },
      "source": [
        "#테스트 데이터 형태소로 분리\n",
        "j = 0\n",
        "for i, row in test_df.iterrows():\n",
        "    val = row['document']\n",
        "    j += 1\n",
        "    if i % 10000 == 0:\n",
        "        print(j,\"/\",\"48000\")\n",
        "    temp = ['CLS']\n",
        "    \n",
        "    for word in api.analyze(val):\n",
        "        for morph in word.morphs:\n",
        "            if morph.lex not in stopwords:\n",
        "                temp.append(morph.lex)\n",
        "    temp.append('SEP')\n",
        "    test_x.append(temp)\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-38cBupadBW"
      },
      "source": [
        "vocab_size = 6000\n",
        "\n",
        "#형태소를 정수로 인코딩\n",
        "tokenizer = Tokenizer(vocab_size,oov_token = 'OOV')\n",
        "with open('/content/gdrive/My Drive/GRUmodel/tokenizer.json') as f:\n",
        "    data = json.load(f)\n",
        "    tokenizer = tokenizer_from_json(data)\n",
        "\n",
        "train_x = tokenizer.texts_to_sequences(train_x)\n",
        "test_x = tokenizer.texts_to_sequences(test_x)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg29ed74jufk"
      },
      "source": [
        "train_y = np.array(train_df['lable'])\n",
        "test_y = np.array(test_df['lable'])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT19Xp2ekb7p"
      },
      "source": [
        "# 제거 후 빈칸만 남은 거 또한 제거\n",
        "drop_train = [index for index, sentence in enumerate(train_x) if len(sentence) < 1]\n",
        "drop_test = [index for index, sentence in enumerate(test_x) if len(sentence) < 1]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOt8np65kePK",
        "outputId": "9a48bd46-83d0-4514-b0cc-98860f067091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_x = np.delete(train_x, drop_train, axis=0)\n",
        "train_y = np.delete(train_y, drop_train, axis=0)\n",
        "print(len(train_x))\n",
        "print(len(train_y))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7104\n",
            "7104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG8qVu8GkxBB",
        "outputId": "6f200ba1-4508-4c2e-e497-5ef5d4e7762a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "test_x = np.delete(test_x, drop_test, axis=0)\n",
        "test_y = np.delete(test_y, drop_test, axis=0)\n",
        "print(len(test_x))\n",
        "print(len(test_y))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "790\n",
            "790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2YrA9wihWdV"
      },
      "source": [
        "#패딩\n",
        "pad_len = 30\n",
        "train_x = pad_sequences(train_x, maxlen = pad_len,padding='post')\n",
        "test_x = pad_sequences(test_x, maxlen = pad_len,padding='post')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THeBVw9LV1GX"
      },
      "source": [
        "#데이터 셋 간단하게\n",
        "class nlp_dataset(Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx]\n",
        "        y = self.y[idx]\n",
        "        return x, y"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zoRdBSmXj5Q"
      },
      "source": [
        "#베이스 라인\n",
        "class grubase(nn.Module):\n",
        "    def __init__(self, embed_dim, vocab_size, hidden_dim, num_layers, batch_size, dropout):\n",
        "        super(grubase, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batch_size = batch_size\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        self.norm = nn.BatchNorm1d(self.batch_size)\n",
        "        self.embed = nn.Embedding(self.vocab_size,self.embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.gru = nn.GRU(self.embed_dim, self.hidden_dim, self.num_layers,bidirectional=True,batch_first=True)\n",
        "        self.gru2 = nn.GRU(self.hidden_dim*2, self.hidden_dim, self.num_layers,batch_first=True)\n",
        "\n",
        "        self.mlp1 = nn.Linear(self.hidden_dim,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.embed(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x, _ = self.gru(x)\n",
        "        x, _ = self.gru2(x)\n",
        "        x = x[:,-1,:]\n",
        "        #x = torch.cat((x[:,0,:],x[:,-1,:]),dim=-1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.mlp1(x)\n",
        "        #x = self.sigmoid(x).squeeze()\n",
        "        return x.squeeze()\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eaZ2j7RkjZF"
      },
      "source": [
        "# 튜닝\n",
        "class grumodel(nn.Module):\n",
        "    def __init__(self, embed_dim, vocab_size, hidden_dim, num_layers, batch_size, dropout):\n",
        "        super(grumodel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batch_size = batch_size\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        self.norm = nn.BatchNorm1d(self.batch_size)\n",
        "        self.embed = nn.Embedding(self.vocab_size,self.embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.gru = nn.GRU(self.embed_dim, self.hidden_dim, self.num_layers,bidirectional=True,batch_first=True)\n",
        "        self.gru2 = nn.GRU(self.hidden_dim*2, self.hidden_dim, self.num_layers,bidirectional=True,batch_first=True)\n",
        "\n",
        "        self.mlp1 = nn.Linear(self.hidden_dim*4,self.hidden_dim)\n",
        "        self.mlp2 = nn.Linear(self.hidden_dim,self.hidden_dim//4)\n",
        "        self.mlp3 = nn.Linear(self.hidden_dim//4,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.embed(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x, _ = self.gru(x)\n",
        "        x, _ = self.gru2(x)\n",
        "        x = torch.cat((x[:,0,:],x[:,-1,:]),dim=-1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.mlp1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.mlp2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.mlp3(x)\n",
        "        return x.squeeze()\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIRES7llXmMk"
      },
      "source": [
        "#학습\n",
        "def train(model, optimizer, loss_function,train_loader,DEVICE):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        x, y = batch\n",
        "        x = x.long().to(DEVICE)\n",
        "        y = y.long().to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(x)\n",
        "        loss = loss_function(y_pred.to(DEVICE).float(), y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkFcl9jlXnKk"
      },
      "source": [
        "# 성능검증 용, F1 스코어, 정확도\n",
        "def getF1(y_pred,y,threshold=0.5):\n",
        "    \n",
        "    yp = [1 if x > threshold else 0 for x in y_pred]\n",
        "\n",
        "    pp = 0\n",
        "    pf = 0\n",
        "    fp = 0\n",
        "    ff = 0\n",
        "    for i in range(len(y)):\n",
        "        if y[i] > threshold:\n",
        "            if yp[i] > threshold: pp += 1\n",
        "            else: pf += 1\n",
        "        else:\n",
        "            if yp[i] < threshold: ff += 1\n",
        "            else: fp += 1\n",
        "\n",
        "    precision = pp / (pp + fp + 1e-5) \n",
        "    recall = pp / (pp + ff + 1e-5)\n",
        "    F1 = 2 * precision * recall / (precision + recall + 1e-5)\n",
        "    acc = (pp + ff) / (len(y) + 1e-5)\n",
        "    return F1, acc"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUiJ0Y2JXoQN"
      },
      "source": [
        "# 검증\n",
        "def evaluate(model, val_loader, loss_function, DEVICE, batch_size, threshold):\n",
        "    \"\"\"evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_f1 = 0\n",
        "    total_acc = 0\n",
        "    for batch in val_loader:\n",
        "        x, y = batch\n",
        "        x = x.long().to(DEVICE)\n",
        "        y = y.long().to(DEVICE)\n",
        "        y_pred = model(x)\n",
        "        loss = loss_function(y_pred.to(DEVICE).float(), y.float())\n",
        "        f1, acc = getF1(y_pred,y,threshold)\n",
        "        total_f1 += f1\n",
        "        total_acc += acc\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    size = len(val_loader.dataset) / batch_size\n",
        "    avg_loss = total_loss / size\n",
        "    avg_f1 = total_f1 / size\n",
        "    avg_acc = total_acc / size\n",
        "    return avg_loss, avg_f1, avg_acc"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeAESeh_Xqq8"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "batch_size = 256\n",
        "embed_dim = 368\n",
        "hidden_dim = 512\n",
        "dropout = 0.7\n",
        "layers = 1\n",
        "\n",
        "model = grumodel(embed_dim,vocab_size,hidden_dim,layers,batch_size,dropout)\n",
        "modelPath = '/content/gdrive/My Drive/GRUmodel/Khaiii_gru_model.pt'\n",
        "model.load_state_dict(torch.load(modelPath))\n",
        "model.to(device)\n",
        "loss = nn.BCEWithLogitsLoss(pos_weight = 1.1 * torch.ones([1])).to(device)\n",
        "lr = 0.001\n",
        "threshold = 0.5\n",
        "\n",
        "EPOCHS = 20\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxzKLsBWXzv9"
      },
      "source": [
        "train_dataset = nlp_dataset(train_x,train_y)\n",
        "test_dataset = nlp_dataset(test_x,test_y)\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size,True,drop_last=True)\n",
        "val_loader = DataLoader(test_dataset,batch_size,True,drop_last=True)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otavCgMCWTh-",
        "outputId": "a57a9940-a758-4f6c-b234-7e9dd2e1cc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(train_dataset[0])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([   2,  920,    1, 3080,   21,   17,    5,    1,    1,    8,    1,\n",
            "          4,   15,  648,   11,   25, 1597,  399,  316,    1,  164,   33,\n",
            "          3,    0,    0,    0,    0,    0,    0,    0], dtype=int32), 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHdz0RnOXtjk",
        "outputId": "b97bd54c-b684-4600-c002-93cf44375b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "best_val_f1 = 0\n",
        "for e in range(1, EPOCHS+1):\n",
        "    train(model, optimizer, loss,train_loader,device)\n",
        "    val_loss,val_f1,val_acc = evaluate(model, val_loader, loss, device,batch_size,threshold)\n",
        "\n",
        "    print(\"[Epoch: %d] val loss : %1.5f    val acc :%4.3f    F1 :%4.3f\" % (e, val_loss, val_acc,val_f1))\n",
        "\n",
        "    # F1 성능 지표로 저장\n",
        "    if not best_val_f1 or val_f1 > best_val_f1:\n",
        "        print(\"Best saved\")\n",
        "        torch.save(model.state_dict(), '/content/gdrive/My Drive/GRUmodel/Khaiii_best_model.pt')\n",
        "        best_val_f1 = val_f1"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1] val loss : 0.60340    val acc :0.654    F1 :0.367\n",
            "Best saved\n",
            "[Epoch: 2] val loss : 0.58831    val acc :0.668    F1 :0.404\n",
            "Best saved\n",
            "[Epoch: 3] val loss : 0.57885    val acc :0.680    F1 :0.418\n",
            "Best saved\n",
            "[Epoch: 4] val loss : 0.57814    val acc :0.682    F1 :0.415\n",
            "[Epoch: 5] val loss : 0.56246    val acc :0.697    F1 :0.452\n",
            "Best saved\n",
            "[Epoch: 6] val loss : 0.56873    val acc :0.689    F1 :0.441\n",
            "[Epoch: 7] val loss : 0.55995    val acc :0.687    F1 :0.442\n",
            "[Epoch: 8] val loss : 0.55670    val acc :0.692    F1 :0.440\n",
            "[Epoch: 9] val loss : 0.55933    val acc :0.691    F1 :0.449\n",
            "[Epoch: 10] val loss : 0.55930    val acc :0.692    F1 :0.445\n",
            "[Epoch: 11] val loss : 0.56514    val acc :0.700    F1 :0.446\n",
            "[Epoch: 12] val loss : 0.57542    val acc :0.700    F1 :0.479\n",
            "Best saved\n",
            "[Epoch: 13] val loss : 0.58104    val acc :0.699    F1 :0.463\n",
            "[Epoch: 14] val loss : 0.59262    val acc :0.700    F1 :0.452\n",
            "[Epoch: 15] val loss : 0.58810    val acc :0.691    F1 :0.457\n",
            "[Epoch: 16] val loss : 0.58967    val acc :0.695    F1 :0.480\n",
            "Best saved\n",
            "[Epoch: 17] val loss : 0.59444    val acc :0.695    F1 :0.443\n",
            "[Epoch: 18] val loss : 0.60786    val acc :0.689    F1 :0.451\n",
            "[Epoch: 19] val loss : 0.61472    val acc :0.697    F1 :0.461\n",
            "[Epoch: 20] val loss : 0.62964    val acc :0.691    F1 :0.464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ZlmpWEL6Lm"
      },
      "source": [
        "model.eval()\n",
        "def predict(original_sentence):\n",
        "    sentence = ['CLS']\n",
        "    for word in api.analyze(original_sentence):\n",
        "        for morph in word.morphs:\n",
        "            if morph.lex not in stopwords:\n",
        "                    sentence.append(morph.lex)\n",
        "    sentence.append('SEP')\n",
        "    x = tokenizer.texts_to_sequences([sentence])\n",
        "    x = pad_sequences(x, maxlen = pad_len,padding='post') # 패딩\n",
        "    x = torch.tensor(x).to(device).long()\n",
        "    \n",
        "    y = model(x)\n",
        "    if(y > 0):\n",
        "        return 'P'\n",
        "    else:\n",
        "        return 'B'"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGGe_MZmqisg",
        "outputId": "23f5a3f5-548c-4dfb-e985-7fa10d9bdf55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "print(predict(\"개소리 오지네\"))\n",
        "print(predict(\"안녕?\"))\n",
        "print(predict(\"아니 씨발\"))\n",
        "print(predict(\"선동 ㄴㄴ\"))\n",
        "print(predict(\"ㅁㅊ ㅅㅂ\"))\n",
        "print(predict(\"ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\"))\n",
        "print(predict(\"존나 웃기네\"))\n",
        "print(predict(\"안녕하세요\"))\n",
        "print(predict(\"개소리를 길게도 쓰셨네요\"))\n",
        "print(predict(\"나가 뒤지세요\"))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B\n",
            "B\n",
            "P\n",
            "P\n",
            "B\n",
            "P\n",
            "P\n",
            "P\n",
            "P\n",
            "P\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQPMNsvlIbQI",
        "outputId": "6891cd0d-ad66-43a1-def1-16c7031f75d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "import json\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/gdrive/My Drive/GRUmodel/tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "'''"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport json\\ntokenizer_json = tokenizer.to_json()\\nwith open('/content/gdrive/My Drive/GRUmodel/tokenizer.json', 'w', encoding='utf-8') as f:\\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    }
  ]
}